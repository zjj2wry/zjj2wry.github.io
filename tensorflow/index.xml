<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tensorflows on zhengjiajin&#39;s blog</title>
    <link>https://zjj2wry.github.io/tensorflow/</link>
    <description>Recent content in Tensorflows on zhengjiajin&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 12 Feb 2019 10:17:16 +0800</lastBuildDate>
    
	<atom:link href="https://zjj2wry.github.io/tensorflow/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>cnn 基本原理</title>
      <link>https://zjj2wry.github.io/tensorflow/cnn/</link>
      <pubDate>Tue, 12 Feb 2019 10:17:16 +0800</pubDate>
      
      <guid>https://zjj2wry.github.io/tensorflow/cnn/</guid>
      <description>为什么要使用 cnn？ 特征提取 cnn 和传统神经网络一样，也是自动提取特征，因为使用了局部感受野(local receptive fields)，cnn 可以表示出相邻的像素点之间的关系，距离较近的像素相关性要远大于距离较远像素的相关性。特征数目过少，我们可能无法精确的分类出来，即我们所说的欠拟合，如果特征数目过多，可能会导致我们在分类过程中过于注重某个特征导致分类错误，即过拟合。卷积层有相当大的先天的对于过度拟合的抵抗。原因是共享权重意味着卷积滤波器被强制从整个图像中学习。这使他们不太可能去选择在训练数据中的局部特质。
平移不变性 cnn 使用 filter 或者 kernel 卷积图像，比如一只猫出现在不同的位置上， 依然可以捕获到相同的特征。另外 cnn 使用了共享权值和偏重，相比于传统的神经网络大大减少了权重和参数的个数。
pooling 层的作用 pooling 层会压缩矩阵，最大的作用是防止过拟合，提高模型泛化能力，图片平移动和旋转后具有不变性</description>
    </item>
    
    <item>
      <title>使用 kubernetes 运行 tensorflow 分布式训练时的常见问题</title>
      <link>https://zjj2wry.github.io/tensorflow/dist/</link>
      <pubDate>Mon, 11 Feb 2019 14:33:25 +0800</pubDate>
      
      <guid>https://zjj2wry.github.io/tensorflow/dist/</guid>
      <description>&lt;h3 id=&#34;worker-执行完任务后没有正常退出-seession-close-失败&#34;&gt;worker 执行完任务后没有正常退出(seession close 失败)&lt;/h3&gt;

&lt;p&gt;tensorflow 分布式训练可以使用 Supervisor 和 MonitoredTrainingSession, 后者是 tensorflow 推荐的方式，使用 tf-operator 运行分布式训练的时候，训练结束后 worker 没有正常退出，因为都是容器，导致的结果是资源无法正常回收。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>